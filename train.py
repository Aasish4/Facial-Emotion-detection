# -*- coding: utf-8 -*-
"""Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JrBU2zDWGI7b_qPzuLqRE-n6kNUNyHzC

# **FACIAL EXPRESSION RECOGNITION**

## DATA PREPROCESSING
"""

# import all the dependencies
import numpy as np
import pandas as pd
import cv2
from sklearn.model_selection import train_test_split

dataset_path = 'data/fer2013.csv'
image_size = (48,48)

# functinon to read the face and the corresponding emotions
def process_dataset(dataset_path):
  data = pd.read_csv(dataset_path)
  pixels = data['pixels'].tolist()
  width, height =  48, 48
  faces = []
  for pix in pixels:
    # iterate over each pixel row and split the row into list removing the space in between
    face = [int(pixel) for pixel in pix.split(' ')]
    # convert the list into numpy array and reshape it into image dim (48,48)
    face = np.asarray(face).reshape(width, height)
    # resize the image into image size
    # type conversion in to uint8 (unsigned integer, values between 0 to 255)
    face = cv2.resize(face.astype('uint8'), image_size)
    faces.append(face.astype('float32'))
  faces = np.asarray(faces)
  faces = np.expand_dims(faces, -1)
  emotions = pd.get_dummies(data['emotion']).to_numpy()
  return faces, emotions

# function for preprocessing the images
# default converts image into range 0 and 1
# version2  converts image into range -1 and 1
def preprocessing(x, version2=True):
  x = x.astype('float32')
  x = x/255.0
  if version2:
    x = x - 0.5
    x = x * 2.0
  return x

faces, emotions = process_dataset('data/fer2013.csv')

faces = preprocessing(faces)

num_samples, num_classes = emotions.shape
xtrain, xtest, ytrain, ytest = train_test_split(faces, emotions, test_size=0.2, shuffle=True)

import matplotlib.pyplot as plt

# display few images
fig , ax = plt.subplots(2, 4, squeeze=False)
ax[0, 0].imshow(faces[0].reshape(image_size), cmap='gray')
ax[0, 0].axis('off')

ax[0, 1].imshow(faces[1].reshape(image_size), cmap='gray')
ax[0, 1].axis('off')

ax[1, 0].imshow(faces[2].reshape(image_size), cmap='gray')
ax[1, 0].axis('off')

ax[1, 1].imshow(faces[3].reshape(image_size), cmap='gray')
ax[1, 1].axis('off')

ax[0, 2].imshow(faces[4].reshape(image_size), cmap='gray')
ax[0, 2].axis('off')

ax[0, 3].imshow(faces[5].reshape(image_size), cmap='gray')
ax[0, 3].axis('off')

ax[1, 2].imshow(faces[6].reshape(image_size), cmap='gray')
ax[1, 2].axis('off')

ax[1, 3].imshow(faces[7].reshape(image_size), cmap='gray')
ax[1, 3].axis('off')
fig.tight_layout()
plt.show()

"""## DATA AUGMENTATION"""

from keras.preprocessing.image import ImageDataGenerator

# define the required image augmentation for better training
data_generator = ImageDataGenerator(
   rotation_range=10,
   width_shift_range=0.1,
   height_shift_range=0.1,
   zoom_range=0.1,
   horizontal_flip=True
)

"""## MODEL DEFINITION"""

from model.model import mini_xception

# model parameters
batch_size= 128
num_epochs = 100
input_shape = (48,48,1)
validation_split = 0.2
verbose = 1
num_classes = 7
patience = 50
base_path = 'model/'

# model object definition
model = mini_xception(input_shape, num_classes)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""# **TRAIN THE MODEL**"""

from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping
from keras.callbacks import ReduceLROnPlateau

# callbacks
log_file_path = base_path + 'emotion_training.log'
csv_logger = CSVLogger(log_file_path, append=False)
early_stop = EarlyStopping('val_loss', patience=patience)
reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)

trained_model_path = base_path + 'mini_xception' + '{epoch:02d}.hdf5'
model_checkpoint = ModelCheckpoint(trained_model_path, 'val_loss', verbose=1, save_best_only=True)

callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]

# start the training
model.fit_generator(data_generator.flow(xtrain, ytrain, batch_size),
                    steps_per_epoch=len(xtrain)/ batch_size,
                    epochs = num_epochs,
                    verbose=1,
                    callbacks=callbacks,
                    validation_data=(xtest, ytest)
                    )

